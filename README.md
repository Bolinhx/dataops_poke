# Projeto de Pipeline de Dados End-to-End com Pok√©mon

![Status](https://img.shields.io/badge/Status-Conclu√≠do-brightgreen?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.11-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Prefect](https://img.shields.io/badge/Prefect-2.19-0052FF?style=for-the-badge&logo=prefect&logoColor=white)
![dbt](https://img.shields.io/badge/dbt-1.8-FF694B?style=for-the-badge&logo=dbt&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-4169E1?style=for-the-badge&logo=postgresql&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-26-2496ED?style=for-the-badge&logo=docker&logoColor=white)

## üìñ Sobre o Projeto

Este projeto demonstra a constru√ß√£o de um pipeline de dados completo (end-to-end), aplicando conceitos modernos de **Engenharia de Dados** e **DataOps**. O objetivo foi simular um ambiente de produ√ß√£o, desde a extra√ß√£o de dados brutos de m√∫ltiplas fontes at√© a cria√ß√£o de um dashboard anal√≠tico interativo.

Utilizando dados do universo Pok√©mon, o pipeline automatiza a ingest√£o, limpeza, transforma√ß√£o, teste e visualiza√ß√£o dos dados, culminando em um dashboard que permite an√°lises aprofundadas sobre os atributos e perfis de combate dos Pok√©mon.

## ‚ú® Destaques

* **Orquestra√ß√£o Robusta:** Uso do **Prefect** para gerenciar o fluxo de ingest√£o, garantindo a execu√ß√£o correta das tarefas, retentativas e observabilidade.
* **Transforma√ß√£o com Analytics Engineering:** Implementa√ß√£o de uma arquitetura de modelagem com **dbt**, separando os dados em camadas (`staging` e `marts`) e aplicando regras de neg√≥cio complexas.
* **Qualidade de Dados Garantida:** Testes automatizados com **dbt test** para validar a integridade, unicidade e consist√™ncia dos dados nos modelos finais.
* **Infraestrutura como C√≥digo:** Uso do **Docker** para provisionar o banco de dados PostgreSQL e a plataforma de BI Metabase, garantindo um ambiente de desenvolvimento reprodut√≠vel.
* **Automa√ß√£o e CI/CD:** Implementa√ß√£o de um workflow com **GitHub Actions** que executa o pipeline completo a cada `push` na branch `main`, garantindo a integra√ß√£o cont√≠nua.
* **Cloud Ready:** O pipeline se conecta a um banco de dados **PostgreSQL na nuvem**, com gerenciamento de segredos atrav√©s de arquivos de ambiente (`.env`).

## üõ†Ô∏è Tech Stack

* **Orquestra√ß√£o de Pipeline:** [Prefect](https://www.prefect.io/)
* **Transforma√ß√£o de Dados:** [dbt (data build tool)](https://www.getdbt.com/)
* **Banco de Dados:** [PostgreSQL](https://www.postgresql.org/) (na nuvem, via Render)
* **Linguagem Principal:** Python
* **Cont√™ineres:** [Docker](https://www.docker.com/)
* **CI/CD:** [GitHub Actions](https://github.com/features/actions)
* **Business Intelligence (BI):** [Metabase](https://www.metabase.com/)

## üìê Arquitetura do Pipeline

```mermaid
graph LR
    A["Fontes de Dados<br/>(Kaggle CSV + PokeAPI)"] --> B["Pipeline de Ingest√£o<br/>(Python + Prefect)"];
    B --> C["Banco de Dados na Nuvem<br/>(PostgreSQL)"];
    C --> D["Transforma√ß√£o e Testes<br/>(dbt build)"];
    D --> E((Dashboard BI<br/>Metabase via Docker));
```

## üìä Preview do Dashboard


![Anima√ß√£o dashboard](https://github.com/user-attachments/assets/397fa4ac-13d8-4ba4-babf-6df269316fb9)



## ‚öôÔ∏è Como Rodar o Projeto

Este projeto foi desenvolvido e testado em um ambiente **WSL 2 (Ubuntu)**.

### Pr√©-requisitos
* WSL 2 com Ubuntu
* Docker Desktop com integra√ß√£o WSL 2 ativada
* Python 3.11+
* Git

### Passos de Instala√ß√£o e Execu√ß√£o

1.  **Crie uma Pasta Limpa (Recomendado):**
    Para evitar problemas com caminhos do Windows, crie o projeto em um local como `C:\dev\`.
    ```bash
    # No terminal WSL
    cd ~
    git clone [https://github.com/Bolinhx/dataops_poke.git](https://github.com/Bolinhx/dataops_poke.git)
    cd dataops_poke
    ```

2.  **Crie e Ative o Ambiente Virtual:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Instale as Depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure os Segredos (Vari√°veis de Ambiente):**
    * Crie uma c√≥pia do arquivo de exemplo: `cp .env.example pokedb.env`.
    * Edite o arquivo `pokedb.env` e preencha com as suas credenciais do banco de dados PostgreSQL.

5.  **Execute o Pipeline de Ingest√£o (Prefect):**
    Este script ir√° ler os dados, chamar a API e popular o banco de dados na nuvem.
    ```bash
    python3 ingestions.py
    ```

6.  **Configure o Perfil do dbt:**
    * Certifique-se de que a pasta `~/.dbt/` existe (rode `dbt debug --config-dir` se necess√°rio para cri√°-la).
    * Edite o arquivo `~/.dbt/profiles.yml` e aponte para as vari√°veis de ambiente, conforme o `profiles.yml.example`.

7.  **Execute a Transforma√ß√£o (dbt):**
    Navegue at√© a pasta do dbt e execute o `build`.
    ```bash
    cd poke_dbt
    dbt build
    ```
    Isso criar√° as views `stg_pokemon` e `mart_pokemon` no seu banco de dados.

8.  **Inicie os Servi√ßos Docker:**
    * No PowerShell, inicie o cont√™iner do Metabase (certifique-se de ter constru√≠do a imagem `meu-metabase-custom` conforme as instru√ß√µes do projeto).
    ```powershell
    docker start meu-metabase-custom # ou o comando docker run
    ```

9.  **Acesse o Dashboard:**
    * Abra o Metabase em **http://localhost:3000** e conecte-o ao seu banco de dados na nuvem.
